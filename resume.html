<!DOCTYPE html>
<html lang="en">
<head profile="http://www.w3.org/2005/10/profile">
    <title>Torsten Scholak</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="description" content="Torsten Scholak&#39;s personal website">
    <meta name="author" content="Torsten Scholak">
    <meta name="keywords" content="AI, ML, Haskell, functional programming">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Open Graph / Facebook / LinkedIn -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Torsten Scholak">
    <meta property="og:description" content="">
    <meta property="og:url" content="https://tscholak.github.io">
    <meta property="og:site_name" content="Torsten Scholak">
    <meta property="article:author" content="Torsten Scholak">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@tscholak">
<meta name="twitter:creator" content="@tscholak">
<meta name="twitter:title" content="Torsten Scholak">
    <meta name="twitter:description" content="">
        <link rel="stylesheet" type="text/css" href="/css/style.css" media="screen" title="default">
    <link rel="stylesheet" type="text/css" href="/css/fonts.css">
    <link rel="stylesheet" type="text/css" href="/css/syntax.css">
</head>
<body>
    <header>
        <nav>
            <a href="/">/</a>
            <a href="/posts">posts/</a>
            <a href="/publications">publications/</a>
            <a href="/tags">tags/</a>
            <a href="/resume">resume/</a>
            <a href="/contact">contact/</a>
            <a href="/terms">terms/</a>
        </nav>
    </header>
    <div>
        <p>commit <a href="https://github.com/tscholak/website/commit/677102f">677102f</a> (2025-08-25 11:06:04 -0400) Torsten Scholak: add json schema validatin</p>

        <h1>Resume</h1>
        <p>Below is my resume. A PDF version is available upon request.</p>
        
            <h2>Torsten Scholak</h2>
            <p>Montréal, QC, Canada</p>
            
            
            <p><a href="https://tscholak.github.io">https://tscholak.github.io</a></p>            
            <p id="csl">
                <span><a href="https://github.com/tscholak">GitHub</a></span>
    <span><a href="https://linkedin.com/in/tscholak">LinkedIn</a></span>
    <span><a href="https://x.com/tscholak">X</a></span>
    <span><a href="https://scholar.google.com/citations?user=BgkjtKgAAAAJ">Google Scholar</a></span>
</p>            
                <h2>Summary</h2>
                <p>AI research and engineering leader turning GPU compute into shipped LLMs. Head of ServiceNow's Foundation Model Lab and co-founder of the ServiceNow Language Model (SLAM) initiative, delivering open-weight models like Apriel-5B and Apriel-Nemotron-15B for efficient enterprise use. I excel at leading small, high-agency teams, setting clear strategy, and executing at scale.</p>
            
            <h2>Experience</h2>
            <h3>Lead Scientist &amp; Team Lead at ServiceNow Research</h3>
            <p>
                Montréal, QC, Canada | 
                2024-01-01
            </p>
            <ul>
                <li>Shipped the open-weights Apriel models, achieving state-of-the-art results on enterprise benchmarks while optimizing for serving cost and latency, recognized by NVIDIA CEO Jensen Huang in the ServiceNow Knowledge 2025 keynote.</li>
    <li>Co-founded the cross-org ServiceNow Language Model (SLAM) initiative, aligning 20+ engineers and researchers on a shared vision for efficient multimodal agentic LLMs that became the Apriel model family.</li>
    <li>Pivoted the Foundation Model Lab from fundamental research to platform-first objectives, shipping 3 model families in 12 months (Mixtral-8x7B multilingual upgrade, Apriel-5B, Apriel-15B) delivering serving-cost and latency wins for the ServiceNow platform.</li>
    <li>Rolled out an "upgrade-don't-retrain" strategy, retrofitting new capabilities (SSMs, multi-token prediction, masked diffusion, pruning, depth/MoE upcycling) via targeted distillation and continual pre-training, adding features at less than 5 % of full token budget and shrinking experiment cycles from weeks to days.</li>
    <li>Set research direction that cut inference latency 4x (2x multi-token prediction + 2x SSMs), slashing serving cost and response time.</li>
    <li>Established critical-path discipline and tight platform alignment, minimizing low-impact work and ensuring on-time delivery, even during a five-week parental leave.</li>
    <li>Coached engineers and researchers into technical leads and fostered a culture of accountability.</li>
    <li>Drove fast resourcing decisions through concise executive updates and clear priority setting.</li>
</ul>            <h3>Applied Research Scientist → Staff Research Scientist at ServiceNow Research</h3>
            <p>
                Montréal, QC, Canada | 
                2021-01 - 2023-12
            </p>
            <ul>
                <li>Pioneered small language models (SLMs) for agentic tasks on the ServiceNow platform.</li>
    <li>Core contributor to the TapeAgents LLM agent development framework.</li>
</ul>            <h3>Applied Research Scientist - Research &amp; AI Core at Element AI (acquired by ServiceNow)</h3>
            <p>
                Montréal, QC, Canada | 
                2017-10 - 2020-12
            </p>
            <ul>
                <li>Tech-led NLP group: set roadmap, ran stand-ups, mentored interns, and liaised with product on dialog systems, text-to-code, summarization, QA.</li>
    <li>Invented grammar-constrained decoding speed-ups (cited 440+ times).</li>
    <li>Built SOTA text-to-SQL model (PICARD) that lead the Spider leaderboard for months.</li>
</ul>            <h3>Data Science Engineer at Unata (acquired by Instacart)</h3>
            <p>
                Toronto, ON, Canada | 
                2016-08 - 2017-09
            </p>
            <ul>
                <li>Re-engineered recommender stack in Scala/Spark.</li>
    <li>Delivered 3.5h Bayesian ML tutorial at PyCon 2017 (12k+ views, recording at https://www.youtube.com/watch?v=fR5Wvb86-IU).</li>
</ul>            <h3>Postdoctoral Researcher at University of Toronto</h3>
            <p>
                Toronto, ON, Canada | 
                2011-06 - 2016-03
            </p>
            <ul>
                <li>Pioneered quantum coherent-control interferometry.</li>
    <li>Ran HPC workloads on SciNet (5 k+ CPU cores, NVIDIA Teslas) to simulate quantum systems.</li>
</ul>            
            <h2>Education</h2>
            <h3>Ph.D. in Theoretical &amp; Mathematical Physics</h3>
            <p>University of Freiburg, Freiburg, Germany</p>
            <p>2008-12 - 2011-12</p>
            <ul>
                <li>magna cum laude</li>
</ul>            <h3>M.S. in Theoretical &amp; Mathematical Physics</h3>
            <p>University of Bayreuth, Bayreuth, Germany</p>
            <p>2002-12 - 2008-12</p>
            <ul>
                <li>GPA: 1.2 (German system, 1.0 is best)</li>
</ul>            
            <h2>Publications</h2>
            <h3>PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models</h3>
            <p id="csl">
                <span>***T. Scholak***</span><span>N. Schucher</span><span>D. Bahdanau</span>
            </p>
            <p>
                EMNLP 2021 | 
                2021-11
                 | <a href="https://doi.org/10.18653/v1/2021.emnlp-main.779">DOI: 10.18653/v1/2021.emnlp-main.779</a>
            </p>
            <p>Incremental parsing keeps generated SQL valid; &gt;400 citations, SOTA on Spider/CoSQL at release.</p>
            <h3>Multilingual Code Retrieval Without Paired Data: A New Benchmark and Experiments</h3>
            <p id="csl">
                <span>J. Monteiro</span><span>***T. Scholak***</span><span>V. Mehta</span><span>D. Vazquez</span><span>C. Pal</span>
            </p>
            <p>
                ICLR 2023 DL4C workshop | 
                2023-03
                
            </p>
            <p>Introduced two cross-lingual code-text datasets; contrastive training beats GPT-4 baselines on 6 languages.</p>
            <h3>StarCoder 2 and The Stack v2: The Next Generation</h3>
            <p id="csl">
                <span>BigCode Team</span>
            </p>
            <p>
                
                2024-02
                 | <a href="https://doi.org/10.48550/arXiv.2402.19173">DOI: 10.48550/arXiv.2402.19173</a>
            </p>
            <p>StarCoder 2 is a 15B parameter model trained on 1.8 trillion tokens of code, achieving SOTA on code generation benchmarks at the time of release.</p>
            <h3>TapeAgents: a Holistic Framework for Agent Development and Optimization</h3>
            <p id="csl">
                <span>D. Bahdanau</span><span>N. Gontier</span><span>G. Huang,</span><span>E. Kamalloo</span><span>R. Pardinas</span><span>A. Piché</span><span>***T. Scholak***</span><span>O. Shliazhko</span><span>J. P. Tremblay</span><span>K. Ghanem</span><span>S. Parikh</span><span>M. Tiwari</span><span>Q. Vohra</span>
            </p>
            <p>
                
                2024-12
                 | <a href="https://doi.org/10.48550/arXiv.2412.08445">DOI: 10.48550/arXiv.2412.08445</a>
            </p>
            <p>Introduces a framework for LLM agent development, including a library of agentic tasks, evaluation metrics, and optimization techniques.</p>
            <h3>Unifying Autoregressive and Diffusion-Based Sequence Generation</h3>
            <p id="csl">
                <span>N. Fathi</span><span>***T. Scholak***</span><span>P.-A. Noel</span>
            </p>
            <p>
                
                2025-04
                 | <a href="https://doi.org/10.48550/arXiv.2504.06416">DOI: 10.48550/arXiv.2504.06416</a>
            </p>
            <p>Shows diffusion can share weights with an AR LM; 2x speed-up at equal perplexity.</p>
            
            <h2>Projects</h2>
            <h3>Apriel Model Family - Co-lead &amp; Technical Architect</h3>
            <p>5-15 B parameter open-weights LLMs optimized for enterprise agentic tasks and efficient inference.</p>
<ul>
                <li>Co-led technical design and release of Apriel-5B and Apriel-Nemotron-15B-Thinker.</li>
    <li>Apriel-5B-Instruct outperforms OLMo-2-7B and Mistral-Nemo-12B across average benchmarks; competitive with LLaMA 3.1 8B, with strong results in math and reasoning (AIME-24/25, GPQA, MATH-500).</li>
    <li>Apriel-Nemotron-15B-Thinker achieves state-of-the-art on BFCL, Enterprise RAG, MT-Bench, MixEval, IFEval, Multi-Challenge, MBPP while using 40% fewer tokens than 30B+ models like QWQ-32B.</li>
    <li>Publicly recognized by NVIDIA CEO Jensen Huang during model announcement in ServiceNow Knowledge 2025 keynote.</li>
</ul>            <h3>Fast-LLM - Strategic Lead</h3>
            <p>Opinionated, high-performance PyTorch-based distributed model-training framework for trillion-token LLM pre-training.</p>
<ul>
                <li>Provided strategic direction and priority setting; day-to-day maintenance handled by core contributors.</li>
    <li>Supports dense, MoE, and hybrid SSM architectures; integrates 3D parallelism, ZeRO, and FlashAttention.</li>
    <li>Adopted by ServiceNow Foundation Model Lab and platform teams for production use.</li>
    <li>Used to train billion-parameter models on trillions of tokens on NVIDIA DGX SuperPOD clusters with 500+ GPUs.</li>
    <li>Trained Apriel-5B and Apriel-Nemotron-15B-Thinker.</li>
</ul>            <h3>Deep Learning for Code (DL4C) - Co-organizer</h3>
            <p>Annual workshop on deep learning for code, co-located with ICLR.</p>
<ul>
                <li>Co-organized 2022 and 2023 workshops, including program committee, event organization, and panel hosting.</li>
    <li>2022: 30+ submissions, 15 accepted papers, 100+ attendees.</li>
    <li>2023: 40+ submissions, 19 accepted papers, 150+ attendees.</li>
</ul>            <h3>Hasktorch - Core Contributor</h3>
            <p>Haskell bindings for PyTorch, enabling GPU-accelerated machine learning in Haskell.</p>
<ul>
                <li>Added compile-time shape/type checking and transformer examples</li>
    <li>Live-coded demo at FP Berlin (8.9k+ views, recording at https://www.youtube.com/watch?v=ZnYa99QoznE&t=1689).</li>
</ul>    </div>

    <footer>
        <p>Copyright © 2025 Torsten Scholak</p>
        <p>
            <a href="/feed.xml"></a>
    
            <a href="https://scholar.google.com/citations?user=BgkjtKgAAAAJ"></a>

            <a href="https://github.com/tscholak"></a>

            <a href="https://twitter.com/tscholak"></a>

            <a href="https://youtube.com/TorstenScholak"></a>

            <a href="https://twitch.com/tscholak"></a>
    </p>
    </footer></body>

</html>